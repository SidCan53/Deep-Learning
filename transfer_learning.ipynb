{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOA0iYpkqR1JG2CqUgpfxPC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SidCan53/Deep-Learning/blob/main/transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0LHrgbaRGTj"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "cCqbLAn5RKsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import ResNet50\n",
        "# load the model\n",
        "model = ResNet50()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CJUMy6AvRNVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Let us load an image to test the pretrained VGG model.\n",
        "#These models are developed on powerful computers so we may as well use them for transfer learning\n",
        "#For VGG16 the images need to be 224x224.\n",
        "from keras.preprocessing.image import load_img\n",
        "image = load_img('/content/drive/MyDrive/FullSizeRender.jpg', target_size=(224, 224))\n",
        "\n",
        "#Convert pixels to Numpy array\n",
        "from keras.preprocessing.image import img_to_array\n",
        "image = img_to_array(image)\n",
        "\n",
        "# Reshape data for the model. VGG expects multiple images of size 224x224x3,\n",
        "#therefore the input shape needs to be (1, 224, 224, 3)\n",
        "#image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "import numpy as np\n",
        "image = np.expand_dims(image, axis=0)\n",
        "\n",
        "#Data needs to be preprocessed same way as the training dataset, to get best results\n",
        "#preprocessing from Keras does this job.\n",
        "#Notice the change in pixel values (Preprocessing subtracts mean RGB value of training set from each pixel)\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "image = preprocess_input(image)\n",
        "\n",
        "\n",
        "# predict the probability across all output categories.\n",
        "#Probability for each of the 1000 classes will be calculated.\n",
        "pred = model.predict(image)\n",
        "\n",
        "#Print the probabilities of the top 5 classes\n",
        "from tensorflow.keras.applications.mobilenet import decode_predictions\n",
        "pred_classes = decode_predictions(pred, top=5)\n",
        "for i in pred_classes[0]:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "KFsdINz2RTuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D, UpSampling2D, Input\n",
        "from keras.models import Sequential, Model\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from skimage.color import rgb2lab, lab2rgb, gray2rgb\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os"
      ],
      "metadata": {
        "id": "FD0-vr_9RWfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Because we are going to replace the encoder part with VGG16,\n",
        "we don’t need it as a classifier, we need it as a feature extractor so,\n",
        "the last dense layers isn’t needed we have to pop them up.\n",
        "\n",
        "here, we iterate on each layer except the last dense layers so,\n",
        "we add 19 layer to our model. the dimension of last layer volume is “7x7x512”.\n",
        "we will be using that latent space volume as a feature vector to be input to the decoder.\n",
        " and the decoder is going to learn the mapping from the latent space vector to ab channels.\n",
        " we want the layers of VGG16 with its original weights without changing them,\n",
        " so that we set the trainable parameter in each layer to false because we don’t want to train them again.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Fo_muWxsRXRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "vggmodel = VGG16()"
      ],
      "metadata": {
        "id": "Om4ZUxzwRafw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newmodel = Sequential()\n",
        "#num = 0\n",
        "for i, layer in enumerate(vggmodel.layers):\n",
        "    if i<19:          #Only up to 19th layer to include feature extraction only\n",
        "      newmodel.add(layer)\n",
        "newmodel.summary()"
      ],
      "metadata": {
        "id": "EHE6FQD7ReBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in newmodel.layers:\n",
        "  layer.trainable=False   #We don't want to train these layers again, so False."
      ],
      "metadata": {
        "id": "5broWFldReWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "VGG16 is expecting an image of 3 dimension with size 224x224 as an input,\n",
        "in preprocessing we have to scale all images to 224 instead of 256\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3bWdySuFRuey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'images/colorization/'\n",
        "#Normalize images - divide by 255\n",
        "train_datagen = ImageDataGenerator(rescale=1. / 255)"
      ],
      "metadata": {
        "id": "iNhhVg-ERxPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_datagen.flow_from_directory(path, target_size=(224, 224), batch_size=32, class_mode=None)"
      ],
      "metadata": {
        "id": "FtoLkrecR1Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert from RGB to Lab\n",
        "\"\"\"\n",
        "by iterating on each image, we convert the RGB to Lab.\n",
        "Think of LAB image as a grey image in L channel and all color info stored in A and B channels.\n",
        "The input to the network will be the L channel, so we assign L channel to X vector.\n",
        "And assign A and B to Y.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "X =[]\n",
        "Y =[]\n",
        "for img in train[0]:\n",
        "  try:\n",
        "      lab = rgb2lab(img)\n",
        "      X.append(lab[:,:,0])\n",
        "      Y.append(lab[:,:,1:] / 128) #A and B values range from -127 to 128,\n",
        "      #so we divide the values by 128 to restrict values to between -1 and 1.\n",
        "  except:\n",
        "     print('error')\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "X = X.reshape(X.shape+(1,)) #dimensions to be the same for X and Y\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "id": "DlzeAa2xR4OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now we have one channel of L in each layer but, VGG16 is expecting 3 dimension,\n",
        "#so we repeated the L channel two times to get 3 dimensions of the same L channel\n",
        "\n",
        "vggfeatures = []\n",
        "for i, sample in enumerate(X):\n",
        "  sample = gray2rgb(sample)\n",
        "  sample = sample.reshape((1,224,224,3))\n",
        "  prediction = newmodel.predict(sample)\n",
        "  prediction = prediction.reshape((7,7,512))\n",
        "  vggfeatures.append(prediction)\n",
        "vggfeatures = np.array(vggfeatures)\n",
        "print(vggfeatures.shape)"
      ],
      "metadata": {
        "id": "tLmCihvnR676"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(256, (3,3), activation='relu', padding='same', input_shape=(7,7,512)))\n",
        "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(16, (3,3), activation='relu', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "vPDn3cw4R9pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='Adam', loss='mse' , metrics=['accuracy'])\n",
        "model.fit(vggfeatures, Y, verbose=1, epochs=10, batch_size=128)\n",
        "\n",
        "model.save('colorize_autoencoder_VGG16.model')"
      ],
      "metadata": {
        "id": "e8plnVYcSDXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "#Predicting using saved model.\n",
        "model = tf.keras.models.load_model('colorize_autoencoder_VGG16_10000.model',\n",
        "                                   custom_objects=None,\n",
        "                                   compile=True)"
      ],
      "metadata": {
        "id": "YQjdGhrUSGYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testpath = 'images/colorization2/test_images/'\n",
        "files = os.listdir(testpath)\n",
        "for idx, file in enumerate(files):\n",
        "    test = img_to_array(load_img(testpath+file))\n",
        "    test = resize(test, (224,224), anti_aliasing=True)\n",
        "    test*= 1.0/255\n",
        "    lab = rgb2lab(test)\n",
        "    l = lab[:,:,0]\n",
        "    L = gray2rgb(l)\n",
        "    L = L.reshape((1,224,224,3))\n",
        "    #print(L.shape)\n",
        "    vggpred = newmodel.predict(L)\n",
        "    ab = model.predict(vggpred)\n",
        "    #print(ab.shape)\n",
        "    ab = ab*128\n",
        "    cur = np.zeros((224, 224, 3))\n",
        "    cur[:,:,0] = l\n",
        "    cur[:,:,1:] = ab\n",
        "    imsave('images/colorization2/vgg_result/result'+str(idx)+\".jpg\", lab2rgb(cur))"
      ],
      "metadata": {
        "id": "joxTDr70SJAo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}